{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4 import UnicodeDammit\n",
    "import http.cookiejar\n",
    "import os \n",
    "import sys\n",
    "import ijson\n",
    "import io\n",
    "from bson.code import Code\n",
    "import json\n",
    "import string\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Scrape Nasdaq to get all the news link, title and dynamic api link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The website url does not get the data by get request, it calls an api which can be found by inspecting\n",
    "headers = {\n",
    "\"Accept\":\"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\",\n",
    "\"Accept-Encoding\":\"gzip, deflate\",\n",
    "\"Accept-Language\":\"en-GB,en;q=0.9,en-US;q=0.8,ml;q=0.7\",\n",
    "\"Connection\":\"keep-alive\",\n",
    "\"Host\":\"www.nasdaq.com\",\n",
    "\"Referer\":\"http://www.nasdaq.com\",\n",
    "\"Upgrade-Insecure-Requests\":\"1\",\n",
    "'User-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.130 Safari/537.36'} \n",
    "\n",
    "result_links=[]\n",
    "result_titles=[]\n",
    "for pg in range(1,50): #page number\n",
    "    api_url = 'https://www.nasdaq.com/api/v1/search?q=tesla&offset='+str((pg-1)*10)  # get dynamic api link\n",
    "    response=requests.get(api_url,headers=headers)\n",
    "    json_result = json.loads(response.content) #the result is a json file \n",
    "    html_result=json_result['items'] #parse the json file \n",
    "    no_of_result=0 #count number of result \n",
    "    for el in html_result:\n",
    "        time.sleep(3) \n",
    "        soup=BeautifulSoup(el,'html.parser')  ##parse the html beautifully\n",
    "        no_of_result=no_of_result+1\n",
    "        #print(no_of_result,\" results\") ##to help debugging\n",
    "        \n",
    "        #find each search result title,link, and timestamp\n",
    "        result_title_tag=soup.select(\"h2.search-result__title>a\")[0] \n",
    "        result_title=result_title_tag.get(\"title\")\n",
    "        partial_link=result_title_tag.get(\"href\") #link is only partial\n",
    "        try:\n",
    "            date_stamp_string=re.search(r'[0-9]{4}-[0-9]{2}-[0-9]{2}',partial_link)[0]\n",
    "            date_stamp=datetime.strptime(date_stamp_string, '%Y-%m-%d') #transfer to date object\n",
    "            date_stamp_bchmark=datetime.strptime('2019-10-01','%Y-%m-%d') #2019.10.01 is the time stock price start to soar\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        #skip the symbol results which are not news\n",
    "        result_eyebrow=soup.select(\"div.search-result__eyebrow\")[0]\n",
    "        #skip the ones that are not news or the ones that are earlier than 2019.10.01\n",
    "        if result_eyebrow.text!=\"Symbols\" and date_stamp>=date_stamp_bchmark: \n",
    "            full_link=\"https://www.nasdaq.com\"+str(partial_link)\n",
    "            result_links.append(full_link)\n",
    "            result_titles.append(result_title)\n",
    "            print(result_title)\n",
    "            print(full_link)\n",
    "    if response.status_code!=200:\n",
    "        raise ValueError(\"Invalid Response Received From Webserver\")\n",
    "    #print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. download the links "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to get the cookie \n",
    "session_requests = requests.session()\n",
    "# going to the home page while logged in  \n",
    "r2=session_requests.get('https://www.nasdaq.com/',headers=headers)\n",
    "cookie=r2.cookies.get_dict()\n",
    "print(\"cookie is :\",cookie)\n",
    "\n",
    "for link in result_links:\n",
    "    ##download each html  GET requests\n",
    "    response=requests.get(link,cookies=cookie,headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser') \n",
    "#   print(soup)\n",
    "#   write file\n",
    "    no_of_news=result_links.index(link)\n",
    "    title=str(result_titles[no_of_news]).translate(str.maketrans(' ','_',string.punctuation))\n",
    "    with open(\"tesla_news_\"+str(no_of_news+1)+\"_\"+str(title)+\".htm\",\"w\",encoding='utf-8') as file:\n",
    "        html_unicode=UnicodeDammit(str(soup)).unicode_markup\n",
    "        file.write(html_unicode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Parse and get the content of the news "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = os.getcwd() #get the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.htm') == False:\n",
    "        continue\n",
    "    try:\n",
    "        with open(os.path.join(directory, filename), 'r',encoding=\"utf-8\") as file:\n",
    "            text = file.read()\n",
    "            soup = BeautifulSoup(text, 'html.parser') \n",
    "            #print(soup)\n",
    "            #get paragraph\n",
    "            paragraphs = soup.findAll('p') \n",
    "        if not paragraphs:\n",
    "            paragraph=None\n",
    "        else:\n",
    "            print(filename)\n",
    "            for el in paragraphs[1:]: #first <p> label is meaningless\n",
    "                if el.text==\"The views and opinions expressed herein are the views and opinions of the author and do not necessarily reflect those of Nasdaq, Inc.\":\n",
    "                    break\n",
    "\n",
    "                print(el.text) \n",
    "    except:\n",
    "        print(\"file \"+filename,sys.exc_info()[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
